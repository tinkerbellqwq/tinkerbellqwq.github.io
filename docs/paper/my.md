# 医学报告生成

## Automatic Radiology Reports Generation via Memory Alignment Network （AAAI 2024）

- 动机：医学影像和文本的跨模态对齐（cross-modal alignment）困难，前人工作有依靠分类器的疾病标签与文本对齐，这依赖分类器的高精度，一旦分类错误将导致后续所有任务；此外，还有人采用了记忆矩阵，作为知识库，所有的图像和文本都可以从中查询特征，从而间接的实现了对齐。本文从记忆矩阵的局限性出发：记忆矩阵存在双重任务，既要学习表征，又要学习对齐，职责不专一，并且效率十分低下，因为它在生成报告时，每生成一个新词都需要重新查询一次记忆矩阵。

- 创新点：

    - 机制创新：提出了一个解耦的记忆对齐（Decoupled Memory Alignment）模块。它将特征表示（Representation）和对齐（Alignment）这两个任务分开，让记忆矩阵只专注于对 - 齐，从而学习更纯粹的跨模态映射关系。
    - 效率创新：对齐过程被设计为一次性计算（One-time Calculation）。在送入BERT生成器之前，所有视觉特征的对齐嵌入可以一次性全部计算完成，与CMN逐词计算的方式形成鲜明对比，极大地提升了推理速度。
    - 信息融合创新：在构建查询（Query）时，巧妙地融合了视觉特征和其对应的位置嵌入（Positional Embedding）。这为对齐模块引入了空间位置的“常识”，使其能更好地理解图像布局，做出更鲁棒的对齐。

- 核心方法：将视觉特征，利用记忆矩阵进行类似注意力操作获取对齐特征，在送入BERT之前，我们可以一次性把所有视觉特征进行对齐，从而实现了高效率。

- 实验结果：


| 数据集 | B@1 | B@2 | B@3 | B@4 | M | R |
| --- | --- | --- | --- | --- | --- | --- |
| MIMIC-CXR | 0.396 | 0.244 | 0.162 | 0.115 | 0.151 | 0.274 |
| IU X-Ray | 0.501 | 0.328 | 0.230 | 0.170 | 0.213 | 0.386 |


---

## DART: Disease-aware Image-Text Alignment and Self-correcting Re-alignment for Trustworthy Radiology Report Generation (CVPR 2025)

- 动机： 现有的放射学报告生成方法存在两大核心痛点。首先，许多依赖检索相似报告的方法，难以保证检索到的报告在疾病层面与输入的X光片真正匹配，因为视觉上的相似不等于病理学上的相似 。其次，绝大多数模型采用“一站式”生成模式，缺乏对已生成报告进行反思和修正的机制，导致报告可能存在事实错误或遗漏。

- 创新点：
    - 疾病感知的对齐机制： 论文创新性地引入了“疾病感知的”对齐方式 。它通过一个疾病分类器提取与疾病直接相关的特征 ，并设计了一个疾病匹配约束（disease-matching constraint, γ） ，强制要求检索到的参考报告必须与输入图像的疾病标签相符，极大地提升了参考信息的可靠性 。
    - 自校正重对齐（Self-Correction）： 本文首次将自校正机制引入放射学报告生成领域 。它设计了一个独立的第二阶段，专门用于修正第一阶段生成的初步报告 。其核心是通过最小化一个修正损失（correction loss, L cor），在嵌入空间中将已生成报告的特征“拉回”到原始图像特征的位置，从而实现内容的精炼和修正 。
    - 两阶段解耦架构： 整个框架被清晰地解构成“初步生成”和“修正重对齐”两个阶段 ，使得每个阶段的任务更纯粹，优化更稳定。

- 核心方法：采用两阶段流程。阶段一，通过对比学习构建图文共享空间 ，并利用疾病分类器和疾病匹配约束来检索高质量的参考报告 ，最终生成一份初步报告。阶段二，将初步报告的特征通过一个自校正模块进行处理，该模块以最小化与原始图像特征的距离为目标 ，产出修正后的特征，并生成最终的、更精确的报告。

- 实验结果：


| 数据集 | B@1 | B@2 | B@3 | B@4 | M | R |
| --- | --- | --- | --- | --- | --- | --- |
| MIMIC-CXR | 0.437 | 0.279 | 0.191 | 0.137 | 0.175 | 0.310 |
| IU X-Ray | 0.486 | 0.348 | 0.265 | 0.208 | 0.205 | 0.411 |

--- 

## Enhanced Contrastive Learning with Multi-view Longitudinal Data for Chest X-ray Report Generation (CVPR 2025)

- 动机： 现有的报告生成模型与临床放射科医生的实际诊断流程存在脱节。医生通常会综合分析多视图（如正位、侧位）影像和纵向（历史）数据来追踪病情变化 。而现有方法大多只处理单视图影像，忽略了空间信息的完整性 ；即便有些方法引入了纵向数据，也仍然只用单张图像分析当前病情，限制了准确性 ；此外，现有模型普遍缺乏对患者数据不完整（如缺少历史报告）情况的鲁棒处理能力。

- 创新点： 
    - 多视图纵向数据融合： 论文的核心创新在于其框架能够灵活地整合来自当前就诊的多视图空间信息和来自历史就诊的纵向时间信息 。为此，它设计了一个专门的多视图纵向融合网络（MLF Network）
    - 多正例对比学习（Multi-positive Contrastive Learning）： 提出了一种新颖的对比学习策略，它将同一次就诊中的所有不同视图图像都视为“正样本对” 。这种方法能有效学习到对于不同拍摄视角的不变性特征，提升了视觉表征的一致性。
    - 标记化缺失编码（Tokenized Absence Encoding）： 为了解决真实世界中数据缺失的问题，论文首创了一种编码技术。它使用[NHI]等特殊字符来显式地表示“INDICATION”等先验知识的缺失，使模型能灵活、鲁棒地处理各种数据不完整的场景。

- 核心方法： 采用两阶段流程。阶段一（预训练），通过多正例对比损失（L_MPC）和跨模态对齐损失（L_G）学习高质量的图文表征。其中，MLF网络负责将多视图和纵向影像融合成统一的时空特征 。阶段二（生成），利用预训练好的模型，通过标记化缺失编码技术处理可能缺失的患者先验知识 ，再将所有信息融合后送入文本生成器（DistilGPT2）产出最终报告。

- 实验结果：


| 数据集 | B@1 | B@2 | B@3 | B@4 | M | R |
| --- | --- | --- | --- | --- | --- | --- |
| MIMIC-CXR | 0.411 | 0.277 | 0.204 | 0.158 | 0.176 | 0.320 |
| Two-view CXR  | 0.417 | 0.276 | 0.200 | 0.154 | 0.178 | 0.331 |
